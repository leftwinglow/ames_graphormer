{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd73ca8-afd1-41ca-943e-a3598a4a9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from networkx import all_pairs_shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d360dec-8745-4fee-adcf-4aeb6c35be95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.data_cleaning import AmesDataset\n",
    "dataset = AmesDataset(\"data\")\n",
    "dataset[0].y.shape[0]\n",
    "\n",
    "# dataset = MoleculeNet(root=\"./\", name=\"ESOL\")\n",
    "# dataset[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dcd5b5d-54ac-4ffe-903e-c788b55cbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch_geometric.nn as tgnn\n",
    "from graphormer.model import Graphormer\n",
    "\n",
    "\n",
    "model = Graphormer(\n",
    "    num_layers=3,\n",
    "    input_node_dim=dataset.num_node_features,\n",
    "    node_dim=128,\n",
    "    input_edge_dim=dataset.num_edge_features,\n",
    "    edge_dim=128,\n",
    "    output_dim=dataset[0].y.shape[0],\n",
    "    n_heads=4,\n",
    "    max_in_degree=5,\n",
    "    max_out_degree=5,\n",
    "    max_path_distance=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0b624d-7d7d-423c-acf2-5c1e0d033917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_ids, train_ids = train_test_split([i for i in range(len(dataset))], test_size=0.8, random_state=42)\n",
    "train_loader = DataLoader(Subset(dataset, train_ids), batch_size=4)\n",
    "test_loader = DataLoader(Subset(dataset, test_ids), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af514e6d-cfd9-46b0-8725-45b422f9db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_functin = nn.L1Loss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f42e66-2ca7-457d-9218-080bd1400ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2428 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m global_mean_pool(model(batch), batch\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_functin(output, y)\n\u001b[0;32m     16\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Luke\\.conda\\envs\\honours\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\model.py:94\u001b[0m, in \u001b[0;36mGraphormer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     91\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_encoding(x, node_paths)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, edge_attr, b, edge_paths, ptr)\n\u001b[0;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_out_lin(x)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Luke\\.conda\\envs\\honours\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\layers.py:245\u001b[0m, in \u001b[0;36mGraphormerEncoderLayer.forward\u001b[1;34m(self, x, edge_attr, b, edge_paths, ptr)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    229\u001b[0m             x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    230\u001b[0m             edge_attr: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    231\u001b[0m             b: torch,\n\u001b[0;32m    232\u001b[0m             edge_paths,\n\u001b[0;32m    233\u001b[0m             ptr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    h′(l) = MHA(LN(h(l−1))) + h(l−1)\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    h(l) = FFN(LN(h′(l))) + h′(l)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    :return: torch.Tensor, node embeddings after Graphormer layer operations\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     x_prime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x), edge_attr, b, edge_paths, ptr) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    246\u001b[0m     x_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x_prime)) \u001b[38;5;241m+\u001b[39m x_prime\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_new\n",
      "File \u001b[1;32mc:\\Users\\Luke\\.conda\\envs\\honours\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\layers.py:197\u001b[0m, in \u001b[0;36mGraphormerMultiHeadAttention.forward\u001b[1;34m(self, x, edge_attr, b, edge_paths, ptr)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m             x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    184\u001b[0m             edge_attr: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    185\u001b[0m             b: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    186\u001b[0m             edge_paths,\n\u001b[0;32m    187\u001b[0m             ptr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    :param x: node feature matrix\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    :param edge_attr: edge feature matrix\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    :return: torch.Tensor, node embeddings after all attention heads\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[1;32m--> 197\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m    198\u001b[0m             attention_head(x, edge_attr, b, edge_paths, ptr) \u001b[38;5;28;01mfor\u001b[39;00m attention_head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\n\u001b[0;32m    199\u001b[0m         ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\layers.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m             x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    184\u001b[0m             edge_attr: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    185\u001b[0m             b: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    186\u001b[0m             edge_paths,\n\u001b[0;32m    187\u001b[0m             ptr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    :param x: node feature matrix\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    :param edge_attr: edge feature matrix\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    :return: torch.Tensor, node embeddings after all attention heads\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[0;32m    197\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m--> 198\u001b[0m             attention_head(x, edge_attr, b, edge_paths, ptr) \u001b[38;5;28;01mfor\u001b[39;00m attention_head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\n\u001b[0;32m    199\u001b[0m         ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Luke\\.conda\\envs\\honours\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\layers.py:147\u001b[0m, in \u001b[0;36mGraphormerAttentionHead.forward\u001b[1;34m(self, x, edge_attr, b, edge_paths, ptr)\u001b[0m\n\u001b[0;32m    144\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk(x)\n\u001b[0;32m    145\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(x)\n\u001b[1;32m--> 147\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_encoding(x, edge_attr, edge_paths)\n\u001b[0;32m    148\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_a(key, query, ptr)\n\u001b[0;32m    149\u001b[0m a \u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m+\u001b[39m c) \u001b[38;5;241m*\u001b[39m batch_mask_neg_inf\n",
      "File \u001b[1;32mc:\\Users\\Luke\\.conda\\envs\\honours\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Luke\\Documents\\University\\5th Year\\ames_graphormer\\graphormer\\layers.py:91\u001b[0m, in \u001b[0;36mEdgeEncoding.forward\u001b[1;34m(self, x, edge_attr, edge_paths)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m edge_paths:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dst \u001b[38;5;129;01min\u001b[39;00m edge_paths[src]:\n\u001b[1;32m---> 91\u001b[0m         path_ij \u001b[38;5;241m=\u001b[39m edge_paths[src][dst][:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_path_distance]\n\u001b[0;32m     92\u001b[0m         weight_inds \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path_ij))]\n\u001b[0;32m     93\u001b[0m         cij[src][dst] \u001b[38;5;241m=\u001b[39m dot_product(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_vector[weight_inds], edge_attr[path_ij])\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model.to(DEVICE)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    batch_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch.to(DEVICE)\n",
    "        y = batch.y\n",
    "        optimizer.zero_grad()\n",
    "        output = global_mean_pool(model(batch), batch.batch)\n",
    "        loss = loss_functin(output, y)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print(\"TRAIN_LOSS\", batch_loss / len(train_ids))\n",
    "\n",
    "    model.eval()\n",
    "    batch_loss = 0.0\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch.to(DEVICE)\n",
    "        y = batch.y\n",
    "        with torch.no_grad():\n",
    "            output = global_mean_pool(model(batch), batch.batch)\n",
    "            loss = loss_functin(output, y)\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    print(\"EVAL LOSS\", batch_loss / len(test_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efd4f9-cc83-42e4-be58-34815323b728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
